{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习纳米学位毕业项目 -- 猫狗大战\n",
    "\n",
    "苗沛\n",
    "\n",
    "2018.08.15\n",
    "\n",
    "**实验环境**\n",
    "\n",
    "- MacBook 10.13.6\n",
    "- python 3.5.4\n",
    "- numpy 1.13.0\n",
    "- tensorflow 1.3.0\n",
    "- Keras 1.2.2\n",
    "- h5py 2.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "从 [Dogs vs. Cats Redux: Kernels Edition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data) 下载训练数据到`image目录` 并解压到当前目录。\n",
    "\n",
    "数据集的文件名是以type.num.jpg方式命名的，比如cat.0.jpg。使用 Keras 的 ImageDataGenerator 需要将不同种类的图片分在不同的文件夹中。对数据集进行预处理参考的是[杨培文的Blog](http://www.zhiding.cn/techwalker/documents/J9UpWRDfVYHE5WsOEHbyx4eM8fBcpHYEW_b72QCUihQ)创建符号链接(symbol link)的方法，这样的好处是不用复制一遍图片，占用不必要的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.chdir(\"{}/image\".format(os.getcwd())) \n",
    "\n",
    "train_filenames = os.listdir('train')\n",
    "train_cat = filter(lambda x:x[:3] == 'cat', train_filenames)\n",
    "train_dog = filter(lambda x:x[:3] == 'dog', train_filenames)\n",
    "\n",
    "def rmrf_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "rmrf_mkdir('img_train')\n",
    "os.mkdir('img_train/cat')\n",
    "os.mkdir('img_train/dog')\n",
    "\n",
    "rmrf_mkdir('img_test')\n",
    "os.symlink('../test/', 'img_test/test')\n",
    "\n",
    "for filename in train_cat:\n",
    "    os.symlink('../../train/'+filename, 'img_train/cat/'+filename)\n",
    "\n",
    "for filename in train_dog:\n",
    "    os.symlink('../../train/'+filename, 'img_train/dog/'+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像文件分类后的路径如下：\n",
    "\n",
    "``` python\n",
    "image/\n",
    "├── test [12500 images]\n",
    "├── img_test\n",
    "│   ├── test -> ../test/\n",
    "├── train [25000 images]\n",
    "├── img_train\n",
    "│   ├── cat [12500 images]\n",
    "│   └── dog [12500 images]\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x = ['train_cat', 'train_dog', 'test']\n",
    "y = [len(os.listdir('img_train/cat')), len(os.listdir('img_train/dog')), len(os.listdir('test'))]\n",
    "ax = sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_count = \"\"\"image数据集中，猫的数量：{}，狗的数量：{}，测试集图片数量：{}\"\"\".format(len(os.listdir('img_train/cat')), len(os.listdir('img_train/dog')),len(os.listdir('test')))\n",
    "s_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导出特征向量\n",
    "\n",
    "对于这个题目来说，使用预训练的网络是最好不过的了，经过前期的测试，我们测试了 ResNet50 等不同的网络，但是排名都不高，现在看来只有一两百名的样子，所以我们需要提高我们的模型表现。那么一种有效的方法是综合各个不同的模型，从而得到不错的效果，兼听则明。如果是直接在一个巨大的网络后面加我们的全连接，那么训练10代就需要跑十次巨大的网络，而且我们的卷积层都是不可训练的，那么这个计算就是浪费的。所以我们可以将多个不同的网络输出的特征向量先保存下来，以便后续的训练，这样做的好处是我们一旦保存了特征向量，即使是在普通笔记本上也能轻松训练。\n",
    "\n",
    "经典的CNN输入图像的尺寸，是224×224、227×227、256×256和299×299，但也可以是其他尺寸。\n",
    "\n",
    "VGG16，VGG19和ResNet均接受224×224输入图像，而Inception V3和Xception需要299×299像素输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "import h5py\n",
    "\n",
    "def write_gap(MODEL, image_size, lambda_func=None):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory(\"img_train\", image_size, shuffle=False, \n",
    "                                              batch_size=16)\n",
    "    test_generator = gen.flow_from_directory(\"img_test\", image_size, shuffle=False, \n",
    "                                             batch_size=16, class_mode=None)\n",
    "\n",
    "    train = model.predict_generator(train_generator, train_generator.nb_sample)\n",
    "    test = model.predict_generator(test_generator, test_generator.nb_sample)\n",
    "\n",
    "    with h5py.File(\"gap_%s.h5\"%MODEL.__name__) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#write_gap(ResNet50, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write_gap(InceptionV3, (299, 299), inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write_gap(Xception, (299, 299), xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入特征向量\n",
    "\n",
    "经过上面的代码以后，我们获得了三个特征向量文件，分别是：\n",
    "\n",
    "- gap_ResNet50.h5\n",
    "- gap_InceptionV3.h5\n",
    "- gap_Xception.h5\n",
    "\n",
    "这里需要载入这些特征向量，并且将它们合成一条特征向量，然后记得把 X 和 y 打乱，不然之后设置validation_split的时候会出问题。这里设置了 numpy 的随机数种子为2018。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(2018)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for filename in [\"gap_ResNet50.h5\", \"gap_Xception.h5\", \"gap_InceptionV3.h5\"]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_test.append(np.array(h['test']))\n",
    "        y_train = np.array(h['label'])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 6144)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "模型的构建很简单，直接 dropout 然后分类就好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1_1:0\", shape=(?, 6144), dtype=float32)\n",
      "Tensor(\"input_1_1:0\", shape=(?, 6144), dtype=float32)\n",
      "Tensor(\"cond_1/Merge:0\", shape=(?, 6144), dtype=float32)\n",
      "Tensor(\"Sigmoid_1:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "input_tensor = Input(X_train.shape[1:])\n",
    "print(input_tensor)\n",
    "x = input_tensor\n",
    "print(x)\n",
    "x = Dropout(0.5)(x)\n",
    "print(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "print(x)\n",
    "model = Model(input_tensor, x)\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对模型进行可视化：\n",
    "\n",
    "进入model_graphviz目录执行make，生成model.png\n",
    "\n",
    "<img src=\"source/model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"source/nnarch1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "模型构件好了以后，我们就可以进行训练了，这里我们设置验证集大小为 20% ，也就是说训练集是20000张图，验证集是5000张图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hist = model.fit(X_train, y_train, batch_size=128, nb_epoch=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型概括\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "model_history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    batch_size=128,\n",
    "                    nb_epoch=8,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks = [TensorBoard(log_dir='./Graph')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir='你存event的目录'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['val_loss'])\n",
    "#plt.xlabel('time')\n",
    "#plt.ylabel('val_loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['val_acc'])\n",
    "#plt.xlabel('times')\n",
    "#plt.ylabel('val_acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# 画图\n",
    "def plot_training(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    epochs = range(len(acc))\n",
    "    plt.plot(epochs, acc, 'r.')\n",
    "    plt.plot(epochs, val_acc, 'r')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.figure()\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']   \n",
    "    plt.plot(epochs, loss, 'r.')\n",
    "    plt.plot(epochs, val_loss, 'r-')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.show()\n",
    "\n",
    "# 训练的acc_loss图\n",
    "plot_training(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测测试集\n",
    "\n",
    "模型训练好以后，我们就可以对测试集进行预测，然后提交到 kaggle 上看看最终成绩了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "image_size = (224, 224)\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"img_test\", image_size, shuffle=False, \n",
    "                                         batch_size=16, class_mode=None)\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.set_value(index-1, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred.csv', index=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测这里我们用到了一个小技巧，我们将每个预测值限制到了 [0.005, 0.995] 个区间内，这个原因很简单，kaggle 官方的评估标准是 LogLoss，对于预测正确的样本，0.995 和 1 相差无几，但是对于预测错误的样本，0 和 0.005 的差距非常大，是 15 和 2 的差别。参考 LogLoss 如何处理无穷大问题，下面的表达式就是二分类问题的 LogLoss 定义。\n",
    "\n",
    "$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right]$$\n",
    "\n",
    "还有一个值得一提的地方就是测试集的文件名不是按 1, 2, 3 这样排的，而是按下面的顺序排列的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls test | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们需要对每个文件名进行处理，然后赋值到 df 里，最后导出为 csv 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "display(data.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow_from_directory(\"img_train\", (224, 224), shuffle=False, batch_size=16)\n",
    "\n",
    "print(train_generator.class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

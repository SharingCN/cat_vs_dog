{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习纳米学位毕业项目 -- 猫狗大战\n",
    "\n",
    "苗沛\n",
    "\n",
    "2018.08.15\n",
    "\n",
    "**实验环境**\n",
    "\n",
    "- MacBook 10.13.6\n",
    "- python 3.5.4\n",
    "- numpy 1.13.0\n",
    "- tensorflow 1.3.0\n",
    "- Keras 1.2.2\n",
    "- h5py 2.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "从 [Dogs vs. Cats Redux: Kernels Edition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data) 下载训练数据到`image目录` 并解压到当前目录。\n",
    "\n",
    "数据集的文件名是以type.num.jpg方式命名的，比如cat.0.jpg。使用 Keras 的 ImageDataGenerator 需要将不同种类的图片分在不同的文件夹中。对数据集进行预处理参考的是[杨培文的Blog](http://www.zhiding.cn/techwalker/documents/J9UpWRDfVYHE5WsOEHbyx4eM8fBcpHYEW_b72QCUihQ)创建符号链接(symbol link)的方法，这样的好处是不用复制一遍图片，占用不必要的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.chdir(\"{}/image\".format(os.getcwd())) \n",
    "\n",
    "train_filenames = os.listdir('train')\n",
    "train_cat = filter(lambda x:x[:3] == 'cat', train_filenames)\n",
    "train_dog = filter(lambda x:x[:3] == 'dog', train_filenames)\n",
    "\n",
    "def rmrf_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "rmrf_mkdir('img_train')\n",
    "os.mkdir('img_train/cat')\n",
    "os.mkdir('img_train/dog')\n",
    "\n",
    "rmrf_mkdir('img_test')\n",
    "os.symlink('../test/', 'img_test/test')\n",
    "\n",
    "for filename in train_cat:\n",
    "    os.symlink('../../train/'+filename, 'img_train/cat/'+filename)\n",
    "\n",
    "for filename in train_dog:\n",
    "    os.symlink('../../train/'+filename, 'img_train/dog/'+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像文件分类后的路径如下：\n",
    "\n",
    "``` python \n",
    "├── test [12500 images]\n",
    "├── test2\n",
    "│   └── test -> ../test/\n",
    "├── train [25000 images]\n",
    "└── train2\n",
    "    ├── cat [12500 images]\n",
    "    └── dog [12500 images]\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x = ['train_cat', 'train_dog', 'test']\n",
    "y = [len(os.listdir('img_train/cat')), len(os.listdir('img_train/dog')), len(os.listdir('test'))]\n",
    "ax = sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_count = \"\"\"image数据集中，猫的数量：{}，狗的数量：{}，测试集图片数量：{}\"\"\".format(len(os.listdir('img_train/cat')), len(os.listdir('img_train/dog')),len(os.listdir('test')))\n",
    "s_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导出特征向量\n",
    "\n",
    "对于这个题目来说，使用预训练的网络是最好不过的了，经过前期的测试，我们测试了 ResNet50 等不同的网络，但是排名都不高，现在看来只有一两百名的样子，所以我们需要提高我们的模型表现。那么一种有效的方法是综合各个不同的模型，从而得到不错的效果，兼听则明。如果是直接在一个巨大的网络后面加我们的全连接，那么训练10代就需要跑十次巨大的网络，而且我们的卷积层都是不可训练的，那么这个计算就是浪费的。所以我们可以将多个不同的网络输出的特征向量先保存下来，以便后续的训练，这样做的好处是我们一旦保存了特征向量，即使是在普通笔记本上也能轻松训练。\n",
    "\n",
    "经典的CNN输入图像的尺寸，是224×224、227×227、256×256和299×299，但也可以是其他尺寸。\n",
    "\n",
    "VGG16，VGG19和ResNet均接受224×224输入图像，而Inception V3和Xception需要299×299像素输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "import h5py\n",
    "\n",
    "def write_gap(MODEL, image_size, lambda_func=None):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory(\"img_train\", image_size, shuffle=False, \n",
    "                                              batch_size=16)\n",
    "    test_generator = gen.flow_from_directory(\"img_test\", image_size, shuffle=False, \n",
    "                                             batch_size=16, class_mode=None)\n",
    "\n",
    "    train = model.predict_generator(train_generator, train_generator.nb_sample)\n",
    "    test = model.predict_generator(test_generator, test_generator.nb_sample)\n",
    "\n",
    "    with h5py.File(\"gap_%s.h5\"%MODEL.__name__) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#write_gap(ResNet50, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write_gap(InceptionV3, (299, 299), inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write_gap(Xception, (299, 299), xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入特征向量\n",
    "\n",
    "经过上面的代码以后，我们获得了三个特征向量文件，分别是：\n",
    "\n",
    "- gap_ResNet50.h5\n",
    "- gap_InceptionV3.h5\n",
    "- gap_Xception.h5\n",
    "\n",
    "这里需要载入这些特征向量，并且将它们合成一条特征向量，然后记得把 X 和 y 打乱，不然之后设置validation_split的时候会出问题。这里设置了 numpy 的随机数种子为2018。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(2018)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for filename in [\"gap_ResNet50.h5\", \"gap_Xception.h5\", \"gap_InceptionV3.h5\"]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_test.append(np.array(h['test']))\n",
    "        y_train = np.array(h['label'])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 6144)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "模型的构建很简单，直接 dropout 然后分类就好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1_1:0\", shape=(?, 6144), dtype=float32)\n",
      "Tensor(\"input_1_1:0\", shape=(?, 6144), dtype=float32)\n",
      "Tensor(\"cond_1/Merge:0\", shape=(?, 6144), dtype=float32)\n",
      "Tensor(\"Sigmoid_1:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "input_tensor = Input(X_train.shape[1:])\n",
    "print(input_tensor)\n",
    "x = input_tensor\n",
    "print(x)\n",
    "x = Dropout(0.5)(x)\n",
    "print(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "print(x)\n",
    "model = Model(input_tensor, x)\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对模型进行可视化：\n",
    "\n",
    "进入model_graphviz目录执行make，生成model.png\n",
    "\n",
    "<img src=\"source/model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"source/nnarch1-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "模型构件好了以后，我们就可以进行训练了，这里我们设置验证集大小为 20% ，也就是说训练集是20000张图，验证集是5000张图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hist = model.fit(X_train, y_train, batch_size=128, nb_epoch=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "model_history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    batch_size=128,\n",
    "                    nb_epoch=8,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks = [TensorBoard(log_dir='./Graph')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir='你存event的目录'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['val_loss'])\n",
    "#plt.xlabel('time')\n",
    "#plt.ylabel('val_loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['val_acc'])\n",
    "#plt.xlabel('times')\n",
    "#plt.ylabel('val_acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# 画图\n",
    "def plot_training(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    epochs = range(len(acc))\n",
    "    plt.plot(epochs, acc, 'r.')\n",
    "    plt.plot(epochs, val_acc, 'r')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.figure()\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']   \n",
    "    plt.plot(epochs, loss, 'r.')\n",
    "    plt.plot(epochs, val_loss, 'r-')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.show()\n",
    "\n",
    "# 训练的acc_loss图\n",
    "plot_training(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测测试集\n",
    "\n",
    "模型训练好以后，我们就可以对测试集进行预测，然后提交到 kaggle 上看看最终成绩了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, verbose=1)\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "image_size = (224, 224)\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"img_test\", image_size, shuffle=False, \n",
    "                                         batch_size=16, class_mode=None)\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.set_value(index-1, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred.csv', index=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测这里我们用到了一个小技巧，我们将每个预测值限制到了 [0.005, 0.995] 个区间内，这个原因很简单，kaggle 官方的评估标准是 LogLoss，对于预测正确的样本，0.995 和 1 相差无几，但是对于预测错误的样本，0 和 0.005 的差距非常大，是 15 和 2 的差别。参考 LogLoss 如何处理无穷大问题，下面的表达式就是二分类问题的 LogLoss 定义。\n",
    "\n",
    "$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right]$$\n",
    "\n",
    "还有一个值得一提的地方就是测试集的文件名不是按 1, 2, 3 这样排的，而是按下面的顺序排列的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls test | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们需要对每个文件名进行处理，然后赋值到 df 里，最后导出为 csv 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "display(data.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow_from_directory(\"img_train\", (224, 224), shuffle=False, batch_size=16)\n",
    "\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(train_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miaopei/anaconda2/envs/cat_vs_dog/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1108: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "\n",
    "model=models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model found in config file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-26f3c5fb1da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_retnet50\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gap_ResNet50.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/miaopei/anaconda2/envs/cat_vs_dog/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No model found in config file."
     ]
    }
   ],
   "source": [
    "model_retnet50=models.load_model('gap_ResNet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7fc57e25af48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip([x.name for x in model.layers], range(len(model.layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_1:0' shape=(?, 6144) dtype=float32>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Sigmoid:0' shape=(?, 1) dtype=float32>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 6144)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 6144)          0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             6145        dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,145\n",
      "Trainable params: 6,145\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 模型概括\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = Model(model.input, [model.layers[2].output, model.output])\n",
    "#model2 = Model(input=model.input, output=model.get_layer(dense_1).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_1:0' shape=(?, 6144) dtype=float32>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_1 to have 2 dimensions, but got array with shape (1, 224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1c2488876272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#print(np.expand_dims(x, axis=0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m#out, prediction = model2.predict(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miaopei/anaconda2/envs/cat_vs_dog/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[1;32m   1253\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                                    check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miaopei/anaconda2/envs/cat_vs_dog/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    110\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_1 to have 2 dimensions, but got array with shape (1, 224, 224, 3)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAI0CAYAAACXhrqLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAewgAAHsIBbtB1PgAAIABJREFUeJzt3X+0XXV95//nm+ANMYJUCU4kola06PijJiJWRLToNAxD\nip2plTWOFsWFrmnLVFGqtiZhHNTWH5mxtqIUf87SWS6pVnGoWJUqTk0kX5d+NTD+rIB3xgsahBRy\nIXnPH599m53r55x7zr13n/vr+VjrrLPPPZ+93/t8cm/O63zO3p8dmYkkSdJ0Ryz0DkiSpMXJkCBJ\nkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKq\nDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqo6DQkRsS4izo6I7RHx\nmYiYiIiDze3KjmqeFxF/GxHjEXF3RPwwIj4UEU/rop4kSctVZGZ3G484OO1H7WIfyMyXzGOto4CP\nA2dNqwMQwEHg0sy8dL5qSpK0nI3i64Zsbj8CPkt5w+7C+zgUED4PnAs8FXgp8F3Ka90aERd0VF+S\npGWl65GErcAuYFdmTkTEw4EfUN7I520kISKeDfxds92/AX4rWy8sIh4M3ACcCPwM+OXMvGM+akuS\ntFx1OpKQmdsz8zOZOdFlHeDVzf0B4D/mtOSTmbcDlzQPjwUcTZAkaQZL/uyGiHgA8OuUUYRrM/PH\nPZpeBfy8WX7eKPZNkqSlbMmHBOAUYKxZvq5Xo8y8F/gHyjERp0TEkSPYN0mSlqzlEBIe11q+cYa2\nU88fCZzUze5IkrQ8LIeQsKG1fMsMbW9uLT+sg32RJGnZWA4h4ejW8l0ztN3XWn5AB/siSdKysRxC\nwlGt5ckZ2u5vLa/pYF8kSVo2lsPBe/e0lsd6tipWt5bvHqZIROxr1k/gpwOscoAyy6MkaeU4Alg1\nQLsHUQ6k35+Za7vdpdlbDiHhztbyTF8htP8hZvpqYrrVHPqHP37IdSVJqlk9c5OFsxxCQvtgxQ3A\n7j5t2wcr3tyzVd0/T9C0bt26GRuvWrWKVasGCZMrx+TkJBMTE6xbt46xsZkGfVRjH86dfTh39mFv\nBw4c4MCBAzO2m5j45zkGu5v2eB4sh5Dw7dbyyZRpmXs5ubm/j3I9h2H8FDh+3bp1/OQnPxlyVQHs\n3r2bTZs2cc0117Bx48aF3p0lyT6cO/tw7uzDuTv++OOngsIgX18vmOVw4OIuDh2weEavRhFxP+Bp\nlNS2KzPvG8G+SZK0ZC35kJCZd1Eu7hTAcyLioT2a/lvgmGb5qlHsmyRJS9miDwkR8eKIONjc3tCj\n2Vub+yOBd0XEYa8rIo4D3tw83Av8VTd7K0nS8tHpMQkRcRqHT398XGv5pIh4cbt9Zn6gz+Z6HtyR\nmV+IiI8CLwB+E7g2InYAPwaeCLyOcpnoBC7xMtGSJM2s6wMXLwBeXPl5AM9oblMS6BcSZvISyuyL\n/xp4FvDsads+AFyamVfMoYYkSSvGKM5uGPT0jn7tZtxGZt4DnBMRLwB+F3gScCzwf4G/B96VmV8d\ncF8kSVrxOg0JmXk+cP4ct/EBhhhhyMyPAh+dS01JkrQEDlyUJEkLw5AwuAOAsyjOwfr169m6dSvr\n169f6F1ZsuzDubMP584+nLvWe8nM0zMuoMhc1DNCLhoRcQtwwgknnMAtt9wyY3tJknrZsGEDt956\nK8CtmblhofenF0cSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIk\nSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElS\nlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUh\nQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGS\nJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRV\nGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkS\nJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVLVyEJCRJwYEW+L\niD0RcVdE3B4ROyPi4ohYM081Hh4Rb46Ir0XEzyJisqlzfUT8SUSsm486kiStBEeOokhEnAN8CDgG\nyObHa4BNwFOACyLi7Mz83hxq/Afg3c12s/XUscDTgF8DLoqIF2Tm52ZbR5KklaLzkYSIeDLwUeBo\n4E7gdcDTgTOB91Le0B8NfDoi1s6yxmnA+4CjgAPAlcC5wFOBfwd8qqnzIOATEfGIWb8gSZJWiFGM\nJOygfLq/F3huZu5sPffFiPgO8GfAY4BXAZfOosYfUQJPAr+XmZe3nrsB+OuIeCvwymZfXgn8wSzq\nSJK0YnQ6khARpwCnU968r5gWEKa8HdgDBOXrgFWzKPX05v72aQGhrR0+fm0WNSRJWlG6/rrh3Nby\n+2sNMjOBDzYPjwWePYs6Y5Qg8oNeDTLz58BtrfaSJKmPrkPCM5r7fZRh/16uay2fNos6N1FGIh7Z\nq0FEHA0c12ovSZL66DokPJbyCf+7mXmwT7sbp60zrHc39w+OiAt7tHlDa/kvZ1FDkqQVpbMDFyNi\nNeWTewK39GubmXsjYh9wf+Bhsyh3JWUE4kXAn0fEJuBvgHHgROCFwPOafXljZn5hFjUkSVpRujy7\n4ejW8l0DtJ8KCQ8YtlAzSnF+RHwKeD1wQXNr+zxwWWZ+ftjtS5K0EnX5dcNRreXJAdrvpxxXMKvZ\nFyPiscCLgSdQRgym355OmbTpobPZviRJK02XIwn3tJYHOZtgNeXN/O5hC0XE6ZSvFx4I/JAymvA5\n4KfAQ4AtwBuBFwDPjIjnZuaeYesATE5Osnv37hnbrV+/nvXr18+mhCRpiRofH2d8fHzGdpOTg3x2\nXnhRzkDsYMPlmIS7KW/8V2fmlhna30n5uuEfMnPgMxwiYgz4PrAe+D/Ar2bmRKXd44CvUcLIDZn5\n1EFrNOvfApwwaPutW7eybdu2YUpIkpa4bdu2sX379mFWuTUzN3S1P3PV2UhCZu6PiNuABwN9OyAi\njgXWUgLFzUOW2gw8tFn3nbWA0OzPtyPiw5RjFTZFxBMy85tD1mLdunVcc801M7ZzFEGSVp4LL7yQ\nLVv6fiYGYPPmzUxMVN+uFpWup2XeQ5lx8aSIOKLPaZAnT1tnGO1TJmf6HuAGDh3QeDIwdEgYGxtj\n48aNw64mSVoBBv2qeWxsaczp1/U8CV9u7tdSrvjYyxmt5euHrHFfa3mm0HO/HutJkqRpug4Jn2gt\nn19rEBFBmd8AYC8w7BwG7amYT5+hbTuM9JzCWZIkdRwSMnMX8CXKqY0vjYhTK80u5tDMjDsy80D7\nyYg4IyIONrcrK+v/HfBPTY1XRMTja/sSEWdRJlSCcqDI12f1oiRJWiFGcanoiyhfIawBro2Iyyij\nBWuA84CXNe1uolwRspfqaRiZeUdEvJlylcdjgK9ExDuBa4GfUU6BPJdyLMLU5aQvmeNrkiRp2es8\nJGTm1yPi+cCHKW/il01vQgkIZ2fmvlnWeGNE/BIlkKwFXtvcpteZBF6bmR+ZTR1JklaSro9JACAz\nrwaeCLyDEgj2UT7l7wJeA2zMzH7HCOS0+1qNVwGnUC729E3g55SDE/dS5kd4O/D4zHzHnF6MJEkr\nRGeTKS03U5MpnXDCCdxyS9/rVUmS1NeGDRu49dZbYZFPpjSSkQRJkrT0GBIkSVKVIUGSJFUZEiRJ\nUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKV\nIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFB\nkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIk\nVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZ\nEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIk\nSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElS\nlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVjSwkRMSJEfG2iNgTEXdFxO0R\nsTMiLo6INfNc68yIeF9EfKeptTciboqIj0XEyyPi/vNZT5Kk5ejIURSJiHOADwHHANn8eA2wCXgK\ncEFEnJ2Z35tjnWOB9wNbmh9l6+mjgUcDvwV8BfjGXGpJkrTcdR4SIuLJwEeBo4A7gcuAL1JCwguA\nl1HevD8dEU/JzH2zrHMM8DlgIyUcXAV8HPgecAB4GHAG8O/m8HIkSVoxRjGSsIMSCO4FnpuZO1vP\nfTEivgP8GfAY4FXApbOs8+eUgHAP8NuZefW053cDnwReGREeiyFJ0gw6fbOMiFOA0ymf7K+YFhCm\nvB3YAwRwUUSsmkWd04AXNnVeXwkIh8nMg8PWkCRppen6E/W5reX31xpkZgIfbB4eCzx7FnV+v7m/\nA3jXLNaXJEnTdB0SntHc7wNu6NPuutbyacMUiIj7UQ5UTODazJxsfn5ERGyIiIdHxOphtilJkroP\nCY+lvHl/d4Yh/hunrTOMJ1EOigT4ZkQcHRE7gNuAHwE/AO6IiM9GxBlDbluSpBWrs5DQfHo/rnl4\nS7+2mbmXMtoA5SyEYTyutbwK+BrwB8ADKQElgfsBzwE+HxGvGXL7kiStSF2OJBzdWr5rgPZTIeEB\nQ9Z5UGv5EuAk4DPAKZQRhuOBVwB7KQdHvqmZt0GSJPXRZUg4qrU8OUD7/ZQ38WFnX1zbWl4NfBY4\nJzN3Z+a9mXl7Zr4HOAeY+srjTUPWkCRpxelynoR7WstjA7RfTflq4O5Z1olm/UuaMyYOk5nXR8RV\nlMmUHhsRj8/M/3/IWkxOTrJ79+4Z261fv57169cPu3lJ0hI2Pj7O+Pj4jO0mJwf57LzwugwJd7aW\nB/kKYWpEYJCvJnrVuS0z+023/LccmnHxFGDokDAxMcGmTZtmbLd161a2bds27OYlSUvY5Zdfzvbt\n2xd6N+ZNZyEhM/dHxG3Ag4EN/do211xYSxkJuHnIUlPtB1m3/fy6IeuUldat45prrpmxnaMIkrTy\nXHjhhWzZsmXGdps3b2ZiYmIEezQ3XU/LvIcy4+JJEXFEn9MgT562zjC+1VqeabbG9vP3DVkHgLGx\nMTZu3DibVSVJy9ygXzWPjQ3yLfzC63qehC8392spV3zspT1/wfXDFMjMH1HmQwjgETM0f1Rr+dZh\n6kiStNJ0HRI+0Vo+v9YgIgJ4UfNwL/CFWdT5eHN/TET8ep92v9Va/nLPVpIkqduQkJm7gC9RPuW/\nNCJOrTS7mEMzM+7IzAPtJyPijIg42Nyu7FFqB4fOcnh7RBw9vUFEvBB4VlPn05npSIIkSX2M4pLJ\nF1FOa7wfcG1E/FFEnBoRz4qIy4G3NO1uolwRspdfOK3xn5/IvBl4AyWMPBHYGRG/GxEbmzrvBN7X\nNP858Mq5vSRJkpa/rg9cJDO/HhHPBz4MHANcNr0JJSCcnZn7pq8/RJ23RsQvUWZdfAwwfdQhgf8D\nnJuZ35ttHUmSVopRjCSQmVdTPuG/gxII9gE/A3YBrwE2ZuYP+m1i2n2vOq+nXEXyQ5QLO91DOc5h\nJ/DHwK9k5s7ZvxJJklaOzkcSpjRfCVzc3IZZ7zpmPrWx3f6rwFeH2ztJkjTdSEYSJEnS0mNIkCRJ\nVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWG\nBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJ\nklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJU\nZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVI\nkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAk\nSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElV\nhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVUjCwkRcWJEvC0i9kTE\nXRFxe0TsjIiLI2JNRzXXRMT3I+Jgc/t+F3UkSVqOjhxFkYg4B/gQcAyQzY/XAJuApwAXRMTZmfm9\neS79n4FHtGpKkqQBdT6SEBFPBj4KHA3cCbwOeDpwJvBeyhv4o4FPR8Taea57EXB3Uzfma9uSJK0E\no/i6YQdl1OA+4LmZ+ZbM/GpmfjEzXw68hvIG/hjgVfNRMCKOoASQI4DLgJ/Nx3YlSVpJOg0JEXEK\ncDpltOCKzNxZafZ2YA8lKFwUEavmofR/AjYCNwJvmYftSZK04nQ9knBua/n9tQaZmcAHm4fHAs+e\nS8GIOBHYTgkmr8jM++ayPUmSVqquQ8Izmvt9wA192l3XWj5tjjX/Erg/8MHM/Ps5bkuSpBWr65Dw\nWMon+u9m5sE+7W6cts6sRMQLgLMoxyC8erbbkSRJHYaEiFgNHNc8vKVf28zcSxltAHjYLOsdC7yD\nEkouyczbZrMdSZJUdDmScHRr+a4B2k+FhAfMst5bgYcAX8nMv5rlNiRJUqPLkHBUa3lygPb7KWc4\nDD37YkQ8EzgfuBd4+bDrS5KkX9TljIv3tJbHBmi/mvJVwd3DFImIMeA9zcMdmfmtYdYf1uTkJLt3\n756x3fr161m/fn2XuyJJWmTGx8cZHx+fsd3k5CCfnRdelyHhztbyIF8hTM22OMhXE21/TJmI6UeU\nUx87NTExwaZNm2Zst3XrVrZt29b17kiSFpHLL7+c7ds7fysamc5CQmbuj4jbgAcDG/q1bQ46XEsZ\nSbh5yFKvadb7HHBORHX25akAsjYifqdZ/klmfmHIWqxbt45rrrlmxnaOIkjSynPhhReyZcuWGdtt\n3ryZiYmJEezR3HR9gac9lBkXT4qII/qcBnnytHWGMfVVxkuaWz/rgI80y18Ehg4JY2NjbNy4cdjV\nJEkrwKBfNY+NDfIt/MLrep6ELzf3aylXfOzljNby9UPWyAFvtbaSJKmHrkPCJ1rL59caRPl+4EXN\nw70M+ek+M1fNdKMcrwDwj62fnznsi5EkaSXpNCRk5i7gS5RTG18aEadWml3MoZkZd2TmgfaTEXFG\nRBxsbld2ub+SJOmQro9JALiI8hXCGuDaiLiMMlqwBjgPeFnT7ibKFSF78esBSZJGqPOQkJlfj4jn\nAx8GjgEum96EEhDOzsx909eXJEkLo+tjEgDIzKuBJ1KurXATZQrmnwG7KKcwbszMH/TbxLT7We3G\nHNeXJGlFGcXXDQBk5s2U4w8uHnK964BVc6z9yLmsL0nSSjSSkQRJkrT0GBIkSVKVIUGSJFUZEiRJ\nUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKV\nIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFB\nkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIk\nVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZ\nEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIk\nSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElSlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElS\nlSFBkiRVGRIkSVKVIUGSJFUZEiRJUpUhQZIkVRkSJElS1chCQkScGBFvi4g9EXFXRNweETsj4uKI\nWDPHba+JiOdFxF802/xpRExGxG0R8ZWI2BoRD5mv1yJJ0kpw5CiKRMQ5wIeAY4BsfrwG2AQ8Bbgg\nIs7OzO/NYttPAL4CrG1+lK2nfwk4FXga8IcR8bLM/NjsXoUkSStL5yMJEfFk4KPA0cCdwOuApwNn\nAu+lvKk/Gvh0RKzttZ0+jqEEhAS+DLwWeC6wEfgN4HLgvqbdf4+I35jL65EkaaUYxUjCDsqowb3A\nczNzZ+u5L0bEd4A/Ax4DvAq4dMjtHwT+B7AtM2+qPP+5iLgG+GtgFfDOppYkSeqj05GEiDgFOJ3y\nKf+KaQFhytuBPUAAF0XEqmFqZOb/yszzegSEqTZ/A1zV1HhURPzqMDUkSVqJuv664dzW8vtrDTIz\ngQ82D48Fnt3RvnyhtfyojmpIkrRsdB0SntHc7wNu6NPuutbyaR3ty+rW8oGOakiStGx0HRIeS/mq\n4buZebBPuxunrdOFM1rLezqqIUnSstFZSIiI1cBxzcNb+rXNzL2U0QaAh3WwL08CzqYElm/0O35B\nkiQVXY4kHN1avmuA9lMh4QHzuRMRMQZcQTmzAeD187l9SZKWqy5DwlGt5ckB2u+nnH0wp9kXK95F\nmbQpgfdn5mfmefuSJC1LXYaEe1rLYwO0X015I797vnYgIl4LvLTZ7k7g9+Zr25IkLXddTqZ0Z2t5\nkK8QpmZbHOSriRlFxIXAf6EEhD3A2Zk55wAyOTnJ7t27Z2y3fv161q9fP9dykqQlZHx8nPHx8Rnb\nTU4OMsC+8DoLCZm5PyJuAx4MbOjXNiKO5dDUyjfPtXZEnEf5miGBH1JmevzpXLcLMDExwaZNm2Zs\nt3XrVrZt2zYfJSVJS8Tll1/O9u3bF3o35k3X0zLvocy4eFJEHNHnNMiTp60zaxGxBfgA5fiGHwNn\nZubMsW5A69at45prrpmxnaMIkrTyXHjhhWzZsmXGdps3b2ZiYmIEezQ3XYeEL1NCwlrKwYO7erRr\nz2Fw/WyLRcSZlOs4HAncRhlB+OFst1czNjbGxo0b53OTkqRlYtCvmsfGBjlUb+F1PZnSJ1rL59ca\nREQAL2oe7uXw6ZMHFhFPb+qtBu4A/lVm3th/LUmS1EunISEzdwFfogz9vzQiTq00u5hDMzPuyMzD\npkyOiDMi4mBzu7JWp7lg06eB+1MOfDwrM78+jy9FkqQVZxSXir6I8hXCGuDaiLiMMlqwBjgPeFnT\n7ibKFSF7ydoPI+KXgWuABzY/+mPgzoj4l3229ZPMXPxfBkmStIA6DwmZ+fWIeD7wYeAY4LLpTSgB\n4ezM3Dd9/QGcDhzferxjgHW2AZfOopYkSStG18ckAJCZVwNPBN5BCQT7gJ9RDmR8DbAxM3/QbxPT\n7mvPD3OTJEkziEzfMwcREbcAJ5xwwgncckvf61VJktTXhg0buPXWWwFuzcy+cwktpJGMJEiSpKXH\nkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAg\nSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmS\nqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoM\nCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmS\nJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSp\nypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQ\nIEmSqgwJkiSpypAgSZKqDAmSJKnKkCBJkqoMCZIkqcqQIEmSqgwJkiSpypAgSZKqDAmSJKlqZCEh\nIk6MiLdFxJ6IuCsibo+InRFxcUSsmcc6Z0XEVRFxc0Tc09xfFRGb56uGJEkrwUhCQkScA3wD+EPg\nMcAa4FhgE/CnwP8XEY+aY42IiCuAq4FzgYcC92vuzwU+ExGXz6HEEQAHDhyYy26uaOPj42zbto3x\n8fGF3pUlyz6cO/tw7uzDuWu9lyzqEf3Ody4ingx8FDgauBN4HfB04EzgvUACjwY+HRFr51DqMuAl\nzfZuAM4Dntrc725+fkFEvHGW218FhoS5GB8fZ/v27f7HMgf24dzZh3NnH85d671k1ULux0yOHEGN\nHZSRg3uB52bmztZzX4yI7wB/RhlheBVw6bAFIuLRzboJ7ALOyMz9zdM3RMSngOuApwCvjoj3Zeb3\nZvuCJElaCTodSYiIU4DTKW/eV0wLCFPeDuwBArgoImaTqv6QQ4Hn91sBAYDMvBv4/ebhkcB/mkUN\nSZJWlK6/bji3tfz+WoPMTOCDzcNjgWfPos4WShC5MTN39ajzVeAmShj5zVnUkCRpRek6JDyjud9H\nOU6gl+tay6cNUyAiHkk5OHH6dvrVOSEiHj5MHUmSVpquQ8JjKZ/wv5uZB/u0u3HaOsN4XI/tzHcd\nSZJWlM6m9mBEAAAPD0lEQVRCQkSsBo5rHt7Sr21m7qWMNgA8bMhSG1rLfesAN7eWh60jSdKK0uVI\nwtGt5bsGaD8VEh7QYZ19reVh60iStKJ0GRKOai1PDtB+P+WgwmFnXxymTvush3mb5VGSpOWoy3kS\n7mktjw3QfjXl+IW7O6yzurU8bJ0HAUxMTHD88cfP2HjVqlWsWrWo58gYucnJkuE2b97M2NggvxKa\nzj6cO/tw7uzD3g4cODDQpHsTExNTiw/qdIfmqMuQcGdreZCh/anZFgf5amK2ddozOg5bJ6YWWv+4\nmgX7b+7sw7mzD+fOPpwXMXOThdNZSMjM/RFxG/BgDj+48BdExLGUN/Dk8IMLB9E+WLFvHQ4/WHHY\nOvs5NNrx0wHaHwD6ndEhSVp+jmCwqZYfRAkI+2dquJC6npZ5D2XGxZMi4og+p0GePG2dYXy7x3bm\ntU5mzuW6EpIkLTldz5Pw5eZ+LeWKj72c0Vq+fpgCmfkD4MeV7dQ8s7m/NTP/cZg6kiStNF2HhE+0\nls+vNYiIAF7UPNwLfGEWdT5JGbY5OSKe2qPO0ygjCTltvyRJUkWnIaG5jsKXKG/gL42IUyvNLubQ\nzIw7MvOww0Ij4oyIONjcruxRagdwX7P8zohonxZJ8/i/NQ/vA/7rrF6QJEkrSNcjCQAXUU43vB9w\nbUT8UUScGhHPiojLgbc07W6iXBGyl+z5ROZ3gLdSwsgpwPUR8fyI2BQRz6d8hfGUZht/6mWiJUma\nWZSLMHZcJOJs4MPAMfzi6R5JCQhnN8cXTF/3DMpXEAl8IDNf0qNGAO8Bpp5v15l6kVdk5oWzfR2S\nJK0koxhJIDOvBp4IvIMSCPYBPwN2Aa8BNtYCQnsT0+5rNTIzXwacTTlG4VbKqSW3No/PMiBIkjS4\nkYwkSJKkpWckIwmSJGnpMSRIkqQqQ4IkSapaUSEhIk6MiLdFxJ6IuCsibo+InRFxcUTM26WjI+Ks\niLgqIm6OiHua+6siYvN81VgoXfZhRKyJiOdFxF802/xpRExGxG0R8ZWI2BoRD5mv17JQRvV7OK3m\nmoj4fmvOke93UWdURtmHEXFmRLwvIr7T1NobETdFxMci4uURcf/5rDcqo+jDiHh4RLw5Ir4WET9r\n/p5vj4jrI+JPImLdfNQZpYhYFxFnR8T2iPhMREwMMJfPXGueFxF/GxHjEXF3RPwwIj7UTBLYrcxc\nETfgHMqMjgcpF19q3w4CNwKPmmONAK5otje9ztTPLl/ovliMfQg8gXJFz1rftWvsBX57oftiMfbh\nDHXfOq3m9xe6LxZ7HwLHUmZn7fU7OfWzJy50nyzGPgT+A+VMtn5/z7cBz1no/hjydR2cdmu/pivn\nudZRwNV9/p3uA97Q6etd6A4f0T/qk5tf1gPAHcAlwKnAs4B3tzp9D7B2DnXe1PrH3AU8n3LNiucD\nX2vVeeNC98li60PgtFbf/T3l1NhfB54EPAf4C2CyaTMJ/MZC98li68MZ6t7b1J56Y1iSIWGEf8vH\nNH+zU7+THwNeQJmsbSPwm5TJ337EEgsJo+jD5u/5vmY79wLvpQSTTcDzKOFr6o3uLuARC90vQ7y2\n9hv2D4H/2frZfIeEj7S2fW2rD38X+N+t5y7o7PUudIeP6B/1uqYz9wNPrTz/qlZnzyqVAY9u3rwO\nAP8ArJ72/BpgZ2s/5v3T4lLuQ+DXmj+IX+nTZkvrj/N/L3SfLLY+7FHzCA4F1NcDP2Bph4SR9CHw\nwWY7/0SZ6K1vHy90vyy2PgQ+1drGhT3atEe3/ttC98sQr20r8K+Bdc3jh3cREoBnt7b71zRTFrSe\nfzAlpBwEbgce2MnrXegOH8E/6Cmtjn5XjzYBfKvV2atmUecvWnVO6dHm1Fabdy503yy2PhxwXz7W\n2pdfXei+Wex9CLyy2d63KJeGX7IhYYR/y+1RrT9c6Ne9RPvw9mb9n/Rpc0xrX3YtdN/MoU+7Cgmf\n4dDI6UN7tPmdVu1XdfH6VsKBi+e2lt9fa5Cltz/YPDyWkuCGtYUyI+SNWS5sVavzVcqMk0EZrlwq\nRtWHg2hfJfRRHdXowsj7MCJOBLZTfi9fkZn3zbDKYjeqPvz95v4O4F2zWH8xG1UfjlF+73rOpJuZ\nP6cckzDVXo2IeADl69YErs3MH/doehXw82b5eV3sy0oICc9o7vcBN/Rpd11r+bRhCkTEI4GHVrbT\nr84JEfHwYeosoM77cAirW8sHerZafBaiD/8SuD/wwcz8+zluazEYxd/y/TgU+K/NzMnm50dExIbm\naP3VfTeyuI3q93Dqw9AjezWIiKOB41rtdcgpHApOPd9TMvNeytfbAZwSEUfO946shJAwdRnq72bm\nwT7tbpy2zjAe12M7811noYyiDwd1Rmt5T0c1ujDSPoyIFwBnUa6R8urZbmeRGUUfPolyRDnANyPi\n6IjYQfnE+yPKJ+M7IuKzzcXnlppR/R6+u7l/cET0umbOG1rLfzmLGsvZbN5TjgROmu8dWdYhoUn8\nU0n1ln5tM3MvJV0DPGzIUhtay33rADe3loetM3Ij7MNB9uVJlAt4JfCNzFwSnz5G3YcRcSzlYmoJ\nXJKZt82wyqI3wj5s/+e8inLQ5x8AD6T0Z1Iue/8c4PMR8Zoht79gRvx7eCXwgWb5zyPiPRHxbyJi\nUzMXyscpB0gm5WyvL/Tc0sq0aN5TlnVIAI5uLd81QPupP4oHdFhnX2t52DoLYVR92FdEjFHmoFjV\n/Oj187n9jo26D98KPAT4Smb+1Sy3sdiMqg8f1Fq+hPLJ7DOU4d+jgOOBV1BOJQ3gTRFxzpA1FsrI\nfg8z82Bmng/8NvAN4ALgbyinhn+c8v3554HnZubWYbe/Aiya95TlHhKOai1PDtB+P+UPf9jZxoap\ns7+13MnsevNsVH04k3dRzg9O4P2Z+Zl53n6XRtaHEfFM4HzKuekvH3b9RWxUfbi2tbwa+CxwTmbu\nzsx7M/P2zHwP5Xz1qeH6Nw1ZY6GM9G85Ih4LvJgyUVpWbk8HLoiIh/bcyMq1aN5TlntIuKe1PMjR\ns6spv7x3d1infdDTsHUWwqj6sKeIeC3w0ma7O4Hfm69tj8hI+rAZbXlP83BHZn5rmPUXuVH/LUdz\nf0lztP9hMvN6ypHlATw2Ih4/ZJ2FMLK/5Yg4HfgKJUzdArwQ+BdN3YcB/5EyB8ULgJ1NoNAhi+Y9\nZbmHhDtby4MMw0x9ihhkKG62ddqfVIatsxBG1YdVzUFP/4Xyn9UeysQ2SyFctY2qD/8YeAzlO8rt\nQ6672C3E3/JtmfmNPm3/trV8ypB1FsJI+rAJqx+hzIMwDpyamR/JzInMPJCZP87MdwPPpLwZrufQ\n8QsqFs17yryfLrGYZOb+iLiNMjPVhn5tm4O91lLejG7u17aifWBJ3zocfmDJsHVGboR9WNveeZSv\nGZIys9hzM/Onc93uqI2wD1/TrPc54JyIqLWZ+g9lbUT8TrP8k8V+4NgI+3Cq/SDrtp9f9BcqGmEf\nbqacEp6USeMmeuzPtyPiw5TjFTZFxBMy85tD1lqupr+n7O7TttP3lOU+kgDl02cAJ0VEv9d78rR1\nhvHtHtuZ7zoLZRR9eJiI2EL5dBHAj4EzM3N8LttcYKPow7Gmxkson+Rqt6mj29e1fvYnQ9ZZKKPo\nw/ZXNKt6tvrF55fKRFWj6MP2Vwf93tzg8LkaZvq/cyWZzXvKfcB353tHVkJI+HJzv5Zy4Fsv7XOe\nrx+mQGb+gPJGNn07Nc9s7m/NzH8cps4C6rwP2yLiTOB/UEa6bqeMIPxwtttbJEbRh7WDw2q3Wtul\nYBR/yz+izIcQwCNmaN6e8fPWYeosoFH8HrYD00yj1ffrsd5Kt4tDByz2fE9pJv96GuVveFcXs6qu\nhJDwidby+bUGUcZlX9Q83MvhU/8O6pOU/1hOjoin9qjzNErqy2n7tdiNqg+JiKc39VZTpsX9V5k5\n02QiS0HnfZiZq2a6Ud4AAf6x9fMzh30xC2RUv4cfb+6PiYhf79Put1rLX+7ZanEZRR+2p2I+fYa2\n7TfAnlM4rzSZeRfwd5T3lOf0OQPk31KO/YByIG0nO7Psbxx+1bNTK8+/mkMXyfiTyvNncOja4dUL\neHD4VSC/Chw17fmjWD5XgeyqD38V+GmzjZ8DT1vo173U+nCAfViyF3gaVR9SvuP9p2YbXweOrrR5\nYavOJxe6XxZTH1ImnrqreX4v8Pge+3EWZfTgIPCjhe6XOfTn0Bd4opwaOtWH1Stt8otXgTxi2vPH\nMYKrQC7rAxdbLqIMma0Bro2IyyjpeA1wHvCypt1NlGvE99JzWDYzvxMRbwX+iHKk8/UR8Rbge5Rh\nyUso13FP4E8z83tzekWj12kfRsQvA9dQ/oOBcqT+nRHxL/ts6yfZ46CoRarz38MVYBR/yzdHxBuA\nPwWeSDlF7y2USYGOoXx6m5qD4ueUK20uJZ32YWbeERFvBi6l9NdXIuKdwLWUacIfQrnQ1AWU0eyk\n/P+4JETEaRw+/fFxreWTIuLF7faZ2e/MjX6/h1+IiI9SThP9Tcq/1Q7KV9tPBF4HnMihmVXvGOqF\nDGqhU9gI097ZlF/QAxxKcFO3A5QDRR7ZY90zGCApUoaG3tu0m15n6meXL3RfLMY+5PBkPehtVte6\nX659OGD9JT2SMMo+pJx6e1+fOj8GnrrQ/bFY+xB4W5/+m1r/HpbY5biB9w3xf9SBHtt4casPev4/\nRhmB/hS931PupTLaM5+3lXBMAgCZeTUlfb2DkpD3Uf5IdlFOHduY5QDEnpuYdl+rkZn5Msof4Ccp\nBzPtb+4/CZyVmb0udrLojaAPBz3wbikdbHeYUfweDrIbc1x/QY2qDzPz9ZQrIH6IEq7uoQyf76SM\ndP1KZu6c/StZOCP6//BVlFHVdwPfpIy63Efpw69RRiken5nvmNOLWRjz8f/UjH+DmXlPZp4D/HvK\nSMz/pbyn/Aj478AzMvM/z/5lzCyatCJJknSYFTOSIEmShmNIkCRJVYYESZJUZUiQJElVhgRJklRl\nSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQ\nJElVhgRJklRlSJAkSVWGBEmSVGVIkCRJVYYESZJUZUiQJElVhgRJklRlSJAkSVX/DzRdqOgKs5/e\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5740434a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 282,
       "width": 260
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.figure(figsize=(12, 14))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    img = cv2.imread('test/%d.jpg' % random.randint(1, 12500))\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    x = img.copy()\n",
    "    x.astype(np.float32)\n",
    "    #print(np.expand_dims(x, axis=0))\n",
    "    out, prediction = model.predict(np.expand_dims(x, axis=0))\n",
    "    #out, prediction = model2.predict(x)\n",
    "\n",
    "    prediction = prediction[0]\n",
    "    out = out[0]\n",
    "\n",
    "    if prediction < 0.5:\n",
    "        plt.title('cat %.2f%%' % (100 - prediction*100))\n",
    "    else:\n",
    "        plt.title('dog %.2f%%' % (prediction*100))\n",
    "\n",
    "    cam = (prediction - 0.5) * np.matmul(out, weights)\n",
    "    cam -= cam.min()\n",
    "    cam /= cam.max()\n",
    "    cam -= 0.2\n",
    "    cam /= 0.8\n",
    "\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    heatmap[np.where(cam <= 0.2)] = 0\n",
    "\n",
    "    out = cv2.addWeighted(img, 0.8, heatmap, 0.4, 0)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(out[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

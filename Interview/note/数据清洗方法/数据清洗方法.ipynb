{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据挖掘中的数据清洗方法大全\n",
    "\n",
    "在数据挖掘领域，经常会遇到的情况是挖掘出来的特征数据存在各种异常情况，如数据缺失 、数据值异常 等。对于这些情况，如果不加以处理，那么会直接影响到最终挖掘模型建立后的使用效果，甚至是使得最终的模型失效，任务失败。所以对于数据挖掘工程师来说，掌握必要的数据清洗方法是很有必要的！\n",
    "\n",
    "接下来本文就依次讲解如何处理 **<font color='red'>数据值缺失</font>** 和 **<font color='red'>数据值异常</font>** 两种情况的处理。\n",
    "\n",
    "## 1.1 缺失值的处理\n",
    "\n",
    "如下图所示，当我们在进行数据挖掘的过程中，往往会出现如下图所示的情况：某些样本的个别属性出现缺失的情况。\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "### 1.1.1 删除缺失值\n",
    "\n",
    "> 当样本数很多的时候，并且出现缺失值的样本在整个的样本的比例相对较小，这种情况下，我们可以使用最简单有效的方法处理缺失值的情况。那就是将出现有缺失值的样本直接丢弃。这是一种很常用的策略。\n",
    "\n",
    "**<font color:'red'>缺点：</font>** 改变了样本的数据分布，并且对于缺失值过多的情况无法适用。\n",
    "\n",
    "### 1.1.2 均值填补法\n",
    "\n",
    "> 根据缺失值的属性相关系数最大的那个属性把数据分成几个组，然后分别计算每个组的均值，把这些均值放入到缺失的数值里面就可以了。\n",
    "\n",
    "**<font color:'red'>缺点：</font>** 改变了数据的分布，还有就是有的优化问题会对方差优化，这样会让对方差优化问题变得不准确。\n",
    "\n",
    "### 1.1.3 热卡填补法\n",
    "\n",
    "> 对于一个包含缺失值的变量，热卡填充法的做法是：在数据库中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。不同的问题可能会选用不同的标准来对相似进行判定。最常见的是使用相关系数矩阵来确定哪个变量（如变量Y）与缺失值所在变量（如变量X）最相关。然后把所有变量按Y的取值大小进行排序。那么变量X的缺失值就可以用排在缺失值前的那个个案的数据来代替了。\n",
    "\n",
    "**<font color:'red'>缺点：</font>** 太麻烦。与均值替换法相比，利用热卡填充法插补数据后，其变量的标准差与插补前比较接近。但在回归方程中，使用热卡填充法容易使得回归方程的误差增大，参数估计变得不稳定，而且这种方法使用不便，比较耗时。\n",
    "\n",
    "### 1.1.4 最近距离决定填补法\n",
    "\n",
    "> 假设现在为时间y,前一段时间为时间x，然后根据x的值去把y的值填补好。\n",
    "\n",
    "缺点： 一般就是在时间因素决定不显著的时候，比如一天的气温，一般不会突然降到很低，然后第二天就升的很高。但是对时间影响比较大的，可能就不可以了。\n",
    "\n",
    "### 1.1.5 回归填补法\n",
    "\n",
    "> 假设我y属性存在部分缺失值，然后我知道x属性。然后我用回归方法对没有缺失的样本进行训练模型，再把这个值的x属性带进去，对这个y属性进行预测，然后填补到缺失处。当然，这里的x属性不一定是一个属性，也可以是一个属性组，这样能够减少单个属性与y属性之间的相关性影响。\n",
    "\n",
    "缺点： 由于是根据x属性预测y属性，这样会让属性之间的相关性变大。这可能会影响最终模型的训练。\n",
    "\n",
    "### 1.1.6 多重填补法（M - 试探法）\n",
    "\n",
    "> 它是基于贝叶斯理论的基础上，然后用EM算法来实现对缺失值进行处理的算法。对每一个缺失值都给M个缺失值，这样数据集就会变成M个，然后用相同的方法对这M个样本集进行处理，得到M个处理结果，总和这M个结果，最终得到对目标变量的估计。其实这个方法很简单，就是我尽量多做模型，然后找出最好的，我就叫它M-试探法吧。\n",
    "\n",
    "### 1.1.7 k-最近邻法\n",
    "\n",
    "> 先根绝欧氏距离和马氏距离函数来确定具有缺失值数据最近的k个元祖，然后将这个k个值加权（权重一般是距离的比值吧）平均来估计缺失值。\n",
    "\n",
    "### 1.1.8 有序最近邻法\n",
    "\n",
    "> 这个方法是在K-最近邻法的基础上，根据属性的缺失率进行排序，从缺失率最小的进行填补。这样做的好处是讲算法处理后的数据也加入到对新的缺失值的计算中，这样即使丢了很多数据，依然会有很好的效果。在这里需要注意的是，欧式距离不考虑各个变量之间的相关性，这样可能会使缺失值的估计不是最佳的情况，所以一般都是用马氏距离进行最近邻法的计算。\n",
    "\n",
    "### 1.1.9 基于贝叶斯的方法\n",
    "\n",
    "> 就是分别将缺失的属性作为预测项，然后根据最简单的贝叶斯方法，对这个预测项进行预测。但是这个方法有一个缺点，就是说不能把之前的预测出来的数据加入到样本集，会丢失一些数据，会影响到预测。所以现在就是对属性值进行重要性排序，然后把重要的先预测出来，在加入新的数据集，再用新的数据集预测第二个重要的属性，这样一直处理到最后为止。\n",
    "\n",
    "## 1.2 异常值的检测与处理\n",
    "\n",
    "。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. 数据清洗方法\n",
    "\n",
    "从两个角度上看，数据清洗一是为了解决数据质量问题，二是让数据更适合做挖掘。不同的目的下分不同的情况，也都有相应的解决方式和方法。下边简略描述一下。\n",
    "\n",
    "---\n",
    "## 1.1 解决数据质量问题\n",
    "\n",
    "解决数据的各种问题，包括但不限于：\n",
    "\n",
    "1. 数据的完整性 --- 例如人的属性中缺少性别、籍贯、年龄等\n",
    "2. 数据的唯一性 --- 例如不同来源的数据出现重复的情况\n",
    "3. 数据的权威性 --- 例如同一个指标出现多个来源的数据，且数值不一样\n",
    "4. 数据的合法性 --- 例如获取的数据与常识不符，年龄大于150岁\n",
    "5. 数据的一致性 --- 例如不同来源数据的不同指标，实际内涵是一样的，或是同一指标内涵不一致\n",
    "\n",
    "数据清洗的结果是对各种脏数据进行对应方式的处理，得到标准的、干净的、连续的数据，提供给数据统计、数据挖掘等使用。\n",
    "\n",
    "那么为了解决以上的各种问题，我们需要不同的手段和方法来一一处理。\n",
    "\n",
    "每种问题都有各种情况，每种情况适用不同的处理方法，具体如下：\n",
    "\n",
    "----\n",
    "### 1.1.1 解决数据的完整性问题\n",
    "\n",
    "> 解决思路：数据缺失，那么补上就好了\n",
    "\n",
    "> 补数据有什么方法：\n",
    "\n",
    "> * 通过其他信息补全，例如使用身份证号码推算性别、籍贯、出生日期、年龄等\n",
    "> * 通过前后数据补全，例如时间序列缺数据了，可以使用前后的均值，缺的多了，可以使用平滑等处理\n",
    "> * 实在补不全的，虽然很可惜，但也必须要剔除。但是不要删掉，没准以后可以用的上\n",
    "\n",
    "---\n",
    "### 1.1.2 解决数据的唯一性问题\n",
    "\n",
    "> 解决思路：去除重复记录，只保留一条\n",
    "\n",
    "> 去重的方法：\n",
    "\n",
    "> * 按主键去重，用 sql 或者 excel “去除重复记录” 即可\n",
    "> * 按规则去重，编写一系列的规则，对重复情况复杂的数据进行去重。例如不同渠道来的客户数据，可以通过相同的关键信息进行匹配，合并去重\n",
    "\n",
    "---\n",
    "### 1.1.3 解决数据的权威性问题\n",
    "\n",
    "> 解决思路：用最权威的那个渠道的数据\n",
    "\n",
    "> 方法：\n",
    "\n",
    "> * 对不同渠道设定权威级别\n",
    "\n",
    "### 1.1.4 解决数据的合法性问题\n",
    "\n",
    "> 解决思路：设定判定规则\n",
    "\n",
    "> * 设定强制合法规则，凡是不在此规则范围内的，强制设为最大值，或者判为无效，剔除\n",
    "> * -- 字段类型合法规则：日期字段格式为 “2018-03-20”\n",
    "> * -- 字段内容合法规则：性别 in （男，女，未知），出生日期 <= 今天\n",
    "> * 设定告警规则，凡是不在此规则范围内的，进行警告，然后人工处理\n",
    "> * -- 警告规则：年龄 > 110\n",
    "> * 离群值人工特殊处理，使用分箱、聚类、回归、等方式发现离群值\n",
    "\n",
    "### 1.1.5 解决数据的一致性问题\n",
    "\n",
    "> 解决思路：简历数据体系，包含但不限于：\n",
    "\n",
    "> * 指标体系（度量）\n",
    "> * 维度（分组，统计口径）\n",
    "> * 单位\n",
    "> * 频度\n",
    "> * 数据\n",
    "\n",
    "## 1.2 让数据更适合做挖掘或展示\n",
    "\n",
    "目标包括但不限于：\n",
    "\n",
    "* 高维度 --- 不适合挖掘\n",
    "* 维度太低 --- 不是合挖掘\n",
    "* 无关信息 --- 减少存储\n",
    "* 字段冗余 --- 一个字段是其他字段计算出来的，会造成相关系数为 1 或者主成因分析异常\n",
    "* 多指标数值、单位不同 --- 如 GDP 与城镇居民人均收入数值相差过大\n",
    "\n",
    "### 1. 2.1 解决高维度问题\n",
    "\n",
    "> 解决思路：降维，方法包括但不限于：\n",
    "\n",
    "> * 主成分分析\n",
    "> * 随机森林\n",
    "\n",
    "### 1.2.2 解决维度低或缺少维度问题\n",
    "\n",
    "> 解决思路：抽象，方法包括但不限于：\n",
    "\n",
    "> * 各种汇总，平均、加总、最大、最小等\n",
    "> * 各种离散化，聚类、自定义分组等\n",
    "\n",
    "### 1.2.3 解决无关信息和字段冗余\n",
    "\n",
    "> 解决思路：剔除字段\n",
    "\n",
    "### 1.2.4 解决多指标数值、单位不同问题\n",
    "\n",
    "> 解决思路：归一化，方法包括但不限于：\n",
    "\n",
    "> * 最大 - 最小\n",
    "> * 零 - 均值\n",
    "> * 小数定标\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[数据挖掘中常用的数据清洗方法有哪些？ 知乎](https://www.zhihu.com/question/22077960)\n",
    "\n",
    "[机器学习基础与实践（一）----数据清洗 系列](http://www.cnblogs.com/charlotte77/p/5606926.html)\n",
    "\n",
    "[数据挖掘中的数据清洗方法大全](http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&mid=2247484528&idx=1&sn=7a432580df83194027431db861fcdd7e&chksm=f9d15ae8cea6d3fe5dc4c933051ba0f638a8aff81d654dc19fbc8849dd8b31f6923144f88e41&mpshare=1&scene=24&srcid=0319u79vXbHAOnusy89P7l9m#rd)\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()\n",
    "\n",
    "[]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
